# Nextwork Teachers TechMonkey â€” Master Plan
## Dual Teacher Live AI Classroom System

---

## ðŸ“‹ Table of Contents

1. [Executive Summary](#executive-summary)
2. [System Overview](#system-overview)
3. [Core Architecture](#core-architecture)
4. [Session State Machine](#session-state-machine)
5. [Event-Driven Communication](#event-driven-communication)
6. [Teacher Pipeline Architecture](#teacher-pipeline-architecture)
7. [RAG Integration](#rag-integration)
8. [Turn-Taking Logic](#turn-taking-logic)
9. [Latency Masking Strategies](#latency-masking-strategies)
10. [Failure Handling & Resilience](#failure-handling--resilience)
11. [Implementation Checklist](#implementation-checklist)
12. [Tech Stack & Infrastructure](#tech-stack--infrastructure)
13. [Future Roadmap](#future-roadmap)

---

## Executive Summary

### TL;DR

You're building a **2-teacher "live classroom"** system where:
- **Teacher A speaks on-camera** while **Teacher B silently prepares** the next clip (RAG â†’ LLM â†’ TTS â†’ Avatar Video)
- Teachers **alternate turns continuously** with zero visible lag
- **Exactly two teachers** are active per session (user-selected)
- Each teacher runs in a **fully independent pipeline**, coordinated by a **session state machine**
- **No merge nodes, no blocking webhooks** â€” synchronize by state + events

### Key Principles

1. **Never block** â€” all operations are asynchronous
2. **Hide latency** â€” renderer always stays one clip ahead
3. **State-driven** â€” synchronize by state + events, never by merging data
4. **Resilient** â€” graceful degradation at every layer
5. **Scalable** â€” independent pipelines enable true concurrency

---

## System Overview

### End Goal: What "Done" Looks Like

#### User Interface Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI Virtual Classroom                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              â”‚                          â”‚                  â”‚
â”‚  Left Panel  â”‚    Center Panel         â”‚   Right Panel    â”‚
â”‚              â”‚                          â”‚                  â”‚
â”‚  Teacher A   â”‚   Website / Lesson       â”‚   Teacher B      â”‚
â”‚  Avatar      â”‚   View (scrollable,      â”‚   Avatar         â”‚
â”‚  (Video)     â”‚   highlightable)         â”‚   (Video)        â”‚
â”‚              â”‚                          â”‚                  â”‚
â”‚  ðŸŽ¤ Speaking â”‚   [Content Area]        â”‚   â³ Rendering   â”‚
â”‚              â”‚                          â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Captions + Controls (pause, next, swap, speed)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Behavioral Requirements

- **Exactly two teachers** active per session (user selects from available pool)
- **Continuous alternation**: While Teacher A is speaking, Teacher B is rendering
- **Zero visible lag**: When A finishes, B's clip is already ready to play
- **Seamless handoffs**: Teachers naturally pass control to each other
- **Context-aware**: Teachers reference visible content, user selections, and RAG-retrieved knowledge

---

## Core Architecture

### Fundamental Rule

> **Never synchronize by merging data. Synchronize by state + events.**

This is the most important architectural decision. It enables:
- True concurrency (both teachers can render simultaneously)
- Clear isolation (each pipeline is independent)
- Predictable behavior (state machine is the single source of truth)
- Easy debugging (events are traceable)

### Three-Component Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Frontend UI                          â”‚
â”‚  â€¢ Teacher pair selector                                   â”‚
â”‚  â€¢ Website container with DOM extraction                    â”‚
â”‚  â€¢ Video/audio playback                                    â”‚
â”‚  â€¢ Event listener (SSE/WebSocket)                          â”‚
â”‚  â€¢ Clip queue management                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â”‚ HTTP/SSE/WebSocket
                        â”‚ â€¢ Start session
                        â”‚ â€¢ Send section snapshots
                        â”‚ â€¢ Receive events
                        â”‚ â€¢ Notify speech-ended
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Coordinator API                          â”‚
â”‚  â€¢ Session state machine (authoritative)                    â”‚
â”‚  â€¢ Turn-taking engine                                      â”‚
â”‚  â€¢ Job routing (enqueue render jobs)                        â”‚
â”‚  â€¢ Event emission (SSE/WebSocket)                          â”‚
â”‚  â€¢ RAG query service                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                       â”‚
        â”‚ enqueue job                           â”‚ enqueue job
        â”‚ (renderer = left)                     â”‚ (renderer = right)
        â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LEFT_WORKER         â”‚              â”‚   RIGHT_WORKER        â”‚
â”‚   (n8n Pipeline)     â”‚              â”‚   (n8n Pipeline)     â”‚
â”‚                      â”‚              â”‚                      â”‚
â”‚ 1. Fetch session     â”‚              â”‚ 1. Fetch session     â”‚
â”‚ 2. Query RAG         â”‚              â”‚ 2. Query RAG         â”‚
â”‚ 3. LLM Generate      â”‚              â”‚ 3. LLM Generate      â”‚
â”‚ 4. TTS Generate      â”‚              â”‚ 4. TTS Generate      â”‚
â”‚ 5. Video Generate    â”‚              â”‚ 5. Video Generate    â”‚
â”‚ 6. POST CLIP_READY   â”‚              â”‚ 6. POST CLIP_READY   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                      â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ POST /session/:id/clip-ready
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Coordinator API          â”‚
              â”‚   â€¢ Updates session state   â”‚
              â”‚   â€¢ Emits CLIP_READY event â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Responsibilities

#### Frontend UI
- **Display**: Render teacher avatars, website content, captions
- **Input**: Capture user interactions (scroll, selection, questions)
- **Communication**: Send snapshots, receive events, notify speech-ended
- **Playback**: Queue and play clips, handle transitions

#### Coordinator API
- **State Management**: Maintain authoritative session state
- **Turn Engine**: Decide who speaks next, swap roles
- **Job Routing**: Enqueue render jobs for the renderer
- **Event Broadcasting**: Emit events to all connected UIs
- **RAG Service**: Query knowledge base, return relevant context

#### n8n Workers (LEFT + RIGHT)
- **Pipeline Execution**: Run complete teacher pipeline independently
- **Validation**: Check if job is still valid before processing
- **Asset Generation**: Create audio, video, captions
- **Notification**: Post CLIP_READY when complete

---

## Session State Machine

### Authoritative Session Object

The Coordinator API maintains the single source of truth for each session:

```json
{
  "sessionId": "abc123",
  "createdAt": "2026-01-25T19:30:00Z",
  "status": "active",
  
  "activeTeachers": ["teacher_a", "teacher_d"],
  "leftTeacher": "teacher_a",
  "rightTeacher": "teacher_d",
  
  "turn": 0,
  "speaker": "teacher_a",
  "renderer": "teacher_d",
  
  "currentSectionId": "sec-05",
  "currentSnapshot": {
    "url": "https://yourproject.com/lesson/5",
    "scrollY": 1280,
    "visibleText": "Step 5: Build the API route...\n\nWe will create...",
    "selectedText": "server-side validation",
    "userQuestion": "Why do we validate on the server?",
    "domDigest": "sha256:abc123...",
    "timestamp": "2026-01-25T19:30:15Z"
  },
  
  "queues": {
    "teacher_a": {
      "status": "speaking",
      "currentClipId": "clip-990",
      "nextClipId": null,
      "lastUpdated": "2026-01-25T19:30:10Z"
    },
    "teacher_d": {
      "status": "ready",
      "currentClipId": null,
      "nextClipId": "clip-991",
      "lastUpdated": "2026-01-25T19:30:20Z"
    }
  },
  
  "history": [
    {
      "turn": 0,
      "speaker": "teacher_a",
      "clipId": "clip-990",
      "timestamp": "2026-01-25T19:30:10Z"
    }
  ]
}
```

### Teacher Status States

Each teacher can be in exactly one of these states:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  idle   â”‚  â† Initial state, no active job
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ enqueue render job
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ rendering  â”‚  â† Worker is generating clip
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ POST CLIP_READY
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ready  â”‚  â† Clip complete, waiting to play
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ swap roles (speaker finishes)
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ speaking â”‚  â† Currently playing on UI
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚ POST speech-ended
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  idle   â”‚  â† Ready for next render job
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Error path:
Any state â†’ error (on failure)
error â†’ idle (after recovery/retry)
```

### Turn-Taking Rules (Invariants)

These rules must **always** be true:

1. **Exactly one speaker**: `speaker âˆˆ activeTeachers`
2. **Exactly one renderer**: `renderer âˆˆ activeTeachers`
3. **Speaker â‰  Renderer**: `speaker !== renderer`
4. **Speaker status**: `queues[speaker].status âˆˆ {speaking, ready}`
5. **Renderer status**: `queues[renderer].status âˆˆ {rendering, ready}`
6. **Swap on completion**: When `speech-ended` received â†’ swap `speaker` and `renderer`

### State Transition Logic Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Session State Machine              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Event: POST /session/start
â”œâ”€ Validate: selectedTeachers.length === 2
â”œâ”€ Create session object
â”œâ”€ Set speaker = leftTeacher (or random)
â”œâ”€ Set renderer = rightTeacher
â”œâ”€ Set queues[speaker].status = "idle"
â”œâ”€ Set queues[renderer].status = "idle"
â””â”€ Enqueue render job for renderer
   â””â”€ POST /worker/{renderer_side}/run

Event: POST /session/:id/section
â”œâ”€ Validate: session exists
â”œâ”€ Update currentSnapshot
â”œâ”€ IF renderer.status === "idle"
â”‚  â””â”€ Enqueue render job for renderer
â””â”€ Emit SECTION_UPDATED event

Event: POST /session/:id/clip-ready
â”œâ”€ Validate: session exists
â”œâ”€ Validate: teacher âˆˆ activeTeachers
â”œâ”€ Validate: teacher === renderer (or allow if ready)
â”œâ”€ Update queues[teacher].status = "ready"
â”œâ”€ Update queues[teacher].nextClipId = clipId
â””â”€ Emit CLIP_READY event
   â””â”€ IF teacher === speaker AND speaker clip not ready
      â””â”€ Use this clip immediately

Event: POST /session/:id/speech-ended
â”œâ”€ Validate: session exists
â”œâ”€ Validate: clipId matches current speaker clip
â”œâ”€ IF renderer.status === "ready"
â”‚  â”œâ”€ Swap: speaker â†” renderer
â”‚  â”œâ”€ Increment turn
â”‚  â”œâ”€ Update queues[new_speaker].status = "speaking"
â”‚  â”œâ”€ Update queues[new_renderer].status = "idle"
â”‚  â”œâ”€ Emit SPEAKER_CHANGED event
â”‚  â””â”€ Enqueue render job for new renderer
â”‚     â””â”€ POST /worker/{new_renderer_side}/run
â””â”€ ELSE (renderer not ready)
   â”œâ”€ Emit WARNING event
   â””â”€ Generate bridging clip for current speaker
      â””â”€ Short filler clip (2-4 seconds)
```

---

## Event-Driven Communication

### Event Stream Protocol

The Coordinator API emits events via **Server-Sent Events (SSE)** or **WebSocket**:

**Connection**: `GET /session/:id/events` (SSE) or `WS /session/:id/events` (WebSocket)

### Event Types

#### 1. SESSION_STARTED

Emitted immediately after session creation.

```json
{
  "type": "SESSION_STARTED",
  "sessionId": "abc123",
  "timestamp": "2026-01-25T19:30:00Z",
  "leftTeacher": "teacher_a",
  "rightTeacher": "teacher_d",
  "speaker": "teacher_a",
  "renderer": "teacher_d",
  "turn": 0
}
```

**UI Action**: Initialize session state, display teacher avatars, start listening for clips.

#### 2. CLIP_READY

Emitted when a worker completes clip generation.

```json
{
  "type": "CLIP_READY",
  "sessionId": "abc123",
  "timestamp": "2026-01-25T19:30:20Z",
  "teacher": "teacher_d",
  "clip": {
    "clipId": "clip-991",
    "text": "Alright, now look at the function on line 42. Notice how we're using server-side validation here...",
    "audioUrl": "http://localhost:8001/audio/clip-991.wav",
    "videoUrl": "http://localhost:8003/video/clip-991.mp4",
    "durationMs": 8200,
    "status": "completed",
    "sectionId": "sec-05",
    "turn": 0,
    "metadata": {
      "onScreenAction": "highlight",
      "targetSelector": "#code-block-3",
      "handoff": "ask_other_teacher"
    }
  }
}
```

**UI Action**: 
- Store clip in teacher's queue
- If teacher === speaker AND no current clip playing â†’ start playing immediately
- If teacher === renderer â†’ wait for speaker to finish

#### 3. SPEAKER_CHANGED

Emitted when roles swap after speech-ended.

```json
{
  "type": "SPEAKER_CHANGED",
  "sessionId": "abc123",
  "timestamp": "2026-01-25T19:30:28Z",
  "speaker": "teacher_d",
  "renderer": "teacher_a",
  "turn": 1,
  "previousSpeaker": "teacher_a",
  "previousRenderer": "teacher_d"
}
```

**UI Action**:
- Stop current speaker's video
- Check if new speaker has ready clip
- If yes â†’ start playing new speaker's clip
- If no â†’ show "Rendering..." message, wait for CLIP_READY

#### 4. SECTION_UPDATED

Emitted when UI sends a new section snapshot.

```json
{
  "type": "SECTION_UPDATED",
  "sessionId": "abc123",
  "timestamp": "2026-01-25T19:30:15Z",
  "sectionId": "sec-05",
  "url": "https://yourproject.com/lesson/5",
  "scrollY": 1280,
  "visibleText": "Step 5: Build the API route...",
  "selectedText": "server-side validation"
}
```

**UI Action**: Update UI to reflect current section (optional visual feedback).

#### 5. ERROR

Emitted when an error occurs.

```json
{
  "type": "ERROR",
  "sessionId": "abc123",
  "timestamp": "2026-01-25T19:30:25Z",
  "teacher": "teacher_d",
  "error": {
    "code": "VIDEO_GENERATION_FAILED",
    "message": "Video generation service unavailable",
    "fallback": "audio_only",
    "clipId": "clip-991-fallback"
  }
}
```

**UI Action**: 
- Show error message to user
- If fallback available â†’ use audio-only clip with idle animation
- Retry or skip based on error type

### Event Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Event Flow                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

UI Action: Start Session
  â”‚
  â–¼
POST /session/start
  â”‚
  â–¼
Coordinator: Create session
  â”‚
  â”œâ”€â–º Emit SESSION_STARTED â”€â”€â–º UI receives â”€â”€â–º Initialize UI
  â”‚
  â””â”€â–º Enqueue render job for renderer
      â”‚
      â–¼
Worker: Generate clip
  â”‚
  â”œâ”€â–º Query RAG
  â”œâ”€â–º LLM Generate
  â”œâ”€â–º TTS Generate
  â”œâ”€â–º Video Generate
  â”‚
  â–¼
POST /session/:id/clip-ready
  â”‚
  â–¼
Coordinator: Update state
  â”‚
  â”œâ”€â–º Emit CLIP_READY â”€â”€â–º UI receives â”€â”€â–º Store in queue
  â”‚
  â””â”€â–º IF speaker clip ready â”€â”€â–º UI plays clip

UI Action: Clip finishes playing
  â”‚
  â–¼
POST /session/:id/speech-ended
  â”‚
  â–¼
Coordinator: Swap roles
  â”‚
  â”œâ”€â–º Emit SPEAKER_CHANGED â”€â”€â–º UI receives â”€â”€â–º Switch to new speaker
  â”‚
  â””â”€â–º Enqueue render job for new renderer
      â”‚
      â””â”€â–º (Loop continues)
```

---

## Teacher Pipeline Architecture

### Pipeline Overview

Each worker (LEFT and RIGHT) runs an identical pipeline template. The only difference is the webhook route and which teacher they're assigned to.

### Complete Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LEFT_WORKER / RIGHT_WORKER Pipeline            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Webhook Trigger
   â”œâ”€ Route: /worker/left/run OR /worker/right/run
   â””â”€ Payload:
      {
        "sessionId": "abc123",
        "teacher": "teacher_a",
        "role": "renderer",
        "sectionPayload": {...},
        "turn": 0
      }

2. HTTP Request: Get Session State
   â”œâ”€ GET /session/:id/state
   â””â”€ Response: Full session object

3. IF Node: Validate Job Still Valid
   â”œâ”€ Condition: teacher âˆˆ session.activeTeachers
   â”œâ”€ Condition: role === session.renderer (or allow if ready)
   â”œâ”€ Condition: turn matches (avoid stale jobs)
   â””â”€ IF invalid â†’ Respond 200 (discard silently)
      IF valid â†’ Continue

4. HTTP Request: Query RAG
   â”œâ”€ POST /rag/query
   â”œâ”€ Payload:
      {
        "sessionId": "abc123",
        "visibleText": "...",
        "selectedText": "...",
        "userQuestion": "...",
        "teacher": "teacher_a"
      }
   â””â”€ Response: Top K relevant chunks with embeddings

5. Code Node: Prepare LLM Prompt
   â”œâ”€ Input: RAG context + sectionPayload + teacher persona
   â”œâ”€ Build prompt:
      {
        "system": "You are Teacher A, co-teaching with Teacher D...",
        "context": "[RAG chunks]",
        "visibleContent": sectionPayload.visibleText,
        "selectedText": sectionPayload.selectedText,
        "userQuestion": sectionPayload.userQuestion,
        "instructions": "Speak in 8-12 second segments, reference on-screen content..."
      }
   â””â”€ Output: Formatted prompt

6. LLM Generate (Ollama/OpenAI)
   â”œâ”€ Model: mistral:7b (or configured model)
   â”œâ”€ Temperature: 0.7
   â”œâ”€ Max tokens: 200
   â””â”€ Response: Raw LLM output

7. Code Node: Extract & Normalize Response
   â”œâ”€ Parse JSON response (if structured)
   â”œâ”€ Extract spoken_text
   â”œâ”€ Validate length (target: 8-12 seconds spoken)
   â”œâ”€ Apply safety filters
   â”œâ”€ Extract metadata (onScreenAction, handoff, etc.)
   â””â”€ Output:
      {
        "text": "Alright, now look at...",
        "durationEstimate": 8500,
        "metadata": {...}
      }

8. HTTP Request: Generate TTS
   â”œâ”€ POST /tts/generate
   â”œâ”€ Payload:
      {
        "text": "...",
        "voice": "teacher_a_voice",
        "language": "en"
      }
   â””â”€ Response:
      {
        "audioUrl": "http://localhost:8001/audio/clip-991.wav",
        "durationMs": 8200
      }

9. HTTP Request: Generate Avatar Video
   â”œâ”€ POST /video/generate
   â”œâ”€ Payload:
      {
        "avatar_id": "teacher_a",
        "audio_url": "http://localhost:8001/audio/clip-991.wav",
        "text_prompt": "A warm educator speaking naturally...",
        "resolution": "480p",
        "num_segments": 1
      }
   â”œâ”€ Timeout: 300000 (5 minutes)
   â”œâ”€ Retries: 2
   â””â”€ Response:
      {
        "job_id": "job-991",
        "status": "processing",
        "video_url": "http://localhost:8003/video/job-991"
      }
   â””â”€ IF error â†’ Continue with audio-only fallback

10. Code Node: Format Clip Object
    â”œâ”€ Input: TTS audioUrl, video job_id, text, metadata
    â”œâ”€ Generate clipId: `clip-{sessionId}-{teacher}-{turn}-{timestamp}`
    â””â”€ Output:
       {
         "clipId": "clip-991",
         "text": "...",
         "audioUrl": "...",
         "videoUrl": "...",
         "jobId": "job-991",
         "durationMs": 8200,
         "status": "processing" | "completed" | "audio_only",
         "sectionId": "sec-05",
         "turn": 0,
         "metadata": {...}
       }

11. HTTP Request: POST Clip Ready
    â”œâ”€ POST /session/:id/clip-ready
    â”œâ”€ Payload:
       {
         "sessionId": "abc123",
         "teacher": "teacher_a",
         "clip": {...}
       }
    â””â”€ Response: {"status": "ok"}

12. Respond to Webhook
    â””â”€ Return 200 OK (job complete)
```

### Pipeline Decision Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Worker Pipeline Decision Tree      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Receive job
  â”‚
  â–¼
Fetch session state
  â”‚
  â–¼
Is job still valid?
  â”œâ”€ NO â†’ Discard silently (return 200)
  â””â”€ YES â†’ Continue
      â”‚
      â–¼
Query RAG
  â”‚
  â”œâ”€ Success â†’ Use RAG context
  â””â”€ Failure â†’ Continue without RAG (log warning)
      â”‚
      â–¼
Generate LLM response
  â”‚
  â”œâ”€ Success â†’ Extract text
  â””â”€ Failure â†’ Retry once, then use fallback text
      â”‚
      â–¼
Generate TTS
  â”‚
  â”œâ”€ Success â†’ Get audioUrl
  â””â”€ Failure â†’ Abort job, send ERROR event
      â”‚
      â–¼
Generate video
  â”‚
  â”œâ”€ Success â†’ Get videoUrl
  â”œâ”€ Timeout â†’ Use audio-only fallback
  â””â”€ Failure â†’ Use audio-only fallback
      â”‚
      â–¼
POST CLIP_READY
  â”‚
  â”œâ”€ Success â†’ Job complete
  â””â”€ Failure â†’ Retry 2x, then log error
```

### No Merge Nodes Policy

**Critical**: The pipeline must never use merge nodes or wait for other pipelines. Each worker is completely independent.

**Why?**
- Enables true concurrency (both teachers can render simultaneously)
- Prevents deadlocks (no waiting on other workers)
- Simplifies debugging (each pipeline is self-contained)
- Allows independent scaling (can run workers on different machines)

---

## RAG Integration

### RAG Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG System                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Knowledge Sources
  â”‚
  â”œâ”€â–º Lesson content (markdown, HTML)
  â”œâ”€â–º Code examples
  â”œâ”€â–º Documentation
  â””â”€â–º Previous session transcripts
      â”‚
      â–¼
Chunking Service
  â”‚
  â”œâ”€â–º Split by semantic boundaries
  â”œâ”€â–º Preserve context (overlap chunks)
  â””â”€â–º Extract metadata (section, lesson, topic)
      â”‚
      â–¼
Embedding Service
  â”‚
  â”œâ”€â–º Generate embeddings (OpenAI/text-embedding-3-small)
  â””â”€â–º Store with metadata
      â”‚
      â–¼
Vector Database (PostgreSQL + pgvector)
  â”‚
  â”œâ”€â–º Store: (embedding, text, metadata, section_id)
  â””â”€â–º Index: HNSW index for fast similarity search
      â”‚
      â–¼
RAG Query Service (Coordinator API)
  â”‚
  â”œâ”€â–º POST /rag/query
  â”œâ”€â–º Input: visibleText, selectedText, userQuestion
  â”œâ”€â–º Generate query embedding
  â”œâ”€â–º Vector similarity search (top K=5)
  â””â”€â–º Return: Relevant chunks with scores
      â”‚
      â–¼
Worker Pipeline
  â”‚
  â””â”€â–º Inject RAG context into LLM prompt
```

### RAG Query Flow

```json
// Request
POST /rag/query
{
  "sessionId": "abc123",
  "visibleText": "Step 5: Build the API route...",
  "selectedText": "server-side validation",
  "userQuestion": "Why do we validate on the server?",
  "teacher": "teacher_a",
  "maxResults": 5
}

// Response
{
  "chunks": [
    {
      "text": "Server-side validation is critical because...",
      "score": 0.92,
      "metadata": {
        "sectionId": "sec-03",
        "lessonId": "lesson-1",
        "topic": "validation"
      }
    },
    {
      "text": "Always validate user input on the server...",
      "score": 0.87,
      "metadata": {...}
    }
  ],
  "queryEmbedding": [0.123, ...],
  "totalResults": 5
}
```

### RAG Prompt Injection

The RAG chunks are injected into the LLM prompt like this:

```
System: You are Teacher A, co-teaching with Teacher D. You are an expert educator
who makes complex topics relatable. Speak in 8-12 second segments.

Context from knowledge base:
1. "Server-side validation is critical because client-side validation can be bypassed..."
2. "Always validate user input on the server to prevent security vulnerabilities..."

Current screen content:
- URL: https://yourproject.com/lesson/5
- Visible text: "Step 5: Build the API route..."
- Selected text: "server-side validation"
- User question: "Why do we validate on the server?"

Instructions:
- Reference the context above when relevant
- Point to specific on-screen elements
- Keep responses to 8-12 seconds when spoken
- End with a handoff cue for Teacher D

Generate your response in JSON format:
{
  "spoken_text": "...",
  "on_screen_action": "highlight|scroll|point|none",
  "target_selector": "#code-block-3",
  "handoff": "ask_other_teacher|continue_self"
}
```

### Pre-Generation & Caching Strategy (Zero-Lag Architecture)

#### Core Concept

To achieve **zero-lag, natural conversation flow**, the system uses a **pre-generation + RAG caching** strategy:

1. **Pre-read lesson** â†’ Parse and split content into logical sections
2. **Pre-assign sections** â†’ Teacher A gets sections 1, 3, 5... Teacher B gets sections 2, 4, 6...
3. **Pre-generate videos** â†’ Each teacher generates video/audio responses for their assigned sections in background
4. **Store in RAG** â†’ Videos, audio, transcripts, and metadata indexed for instant retrieval
5. **Progressive improvement** â†’ More usage = more cached content = faster responses

#### Architecture Flow

```
User selects lesson URL
    â†“
Read & parse lesson content
    â†“
Split into sections (semantic boundaries, not just length)
    â”œâ”€â–º Tag with keywords, concepts, topics
    â””â”€â–º Assign to Teacher A (odd) / Teacher B (even)
    â†“
Pre-generate videos (background, parallel processing)
    â”œâ”€â–º Generate: Video URL, Audio URL, Transcript
    â”œâ”€â–º Extract: Keywords, Concepts, Topics
    â””â”€â–º Store metadata: Section ID, Lesson ID, Teacher, Turn
    â†“
Store in RAG with rich metadata:
    â”œâ”€â–º Video URL (pre-generated)
    â”œâ”€â–º Audio URL (pre-generated)
    â”œâ”€â–º Transcript (full text)
    â”œâ”€â–º Keywords (extracted)
    â”œâ”€â–º Concepts (extracted)
    â”œâ”€â–º Section metadata (ID, lesson, topic)
    â””â”€â–º Embeddings (for similarity search)
    â†“
User asks question
    â†“
RAG searches:
    â”œâ”€â–º Transcript text (semantic similarity)
    â”œâ”€â–º Keywords (exact match)
    â”œâ”€â–º Concepts (conceptual match)
    â””â”€â–º Metadata (section, lesson, topic)
    â†“
Retrieve best match(es) with scores
    â†“
Return pre-generated video/audio (INSTANT - zero lag!)
    â†“
If no match â†’ Fallback to real-time generation
```

#### Two-Tier RAG System

**Tier 1: Pre-Generated Content (Instant)**
- Pre-generated videos/audio for lesson sections
- Common Q&A pairs (pre-answered)
- Frequently accessed content
- **Result**: Zero lag for cached content

**Tier 2: Real-Time Generation (Fallback)**
- For unexpected questions
- Edge cases not covered by pre-generation
- Follow-up questions requiring synthesis
- **Result**: Handles all cases, with slight delay

#### Smart Sectioning Strategy

**Don't just split by length - split by meaning:**

- **Semantic boundaries**: Split at topic changes, concept transitions
- **Natural breaks**: Paragraphs, code blocks, examples
- **Context preservation**: Overlap chunks to maintain context
- **Tagging**: Extract keywords, concepts, topics for each section
- **Result**: Better RAG retrieval, more relevant answers

#### Metadata Indexing Schema

Each pre-generated clip stored in RAG includes:

```json
{
  "clip_id": "clip-{sessionId}-{teacher}-{turn}-{timestamp}",
  "video_url": "http://localhost:8003/video/{jobId}",
  "audio_url": "http://localhost:8001/audio/{audioId}.wav",
  "transcript": "Full spoken text of the clip",
  "section_id": "sec-03",
  "lesson_id": "lesson-1",
  "lesson_url": "https://example.com/lesson/1",
  "teacher": "teacher_a",
  "turn": 2,
  "keywords": ["validation", "server-side", "security"],
  "concepts": ["input validation", "security best practices"],
  "topics": ["backend", "api", "validation"],
  "duration_ms": 8500,
  "created_at": "2026-01-26T00:00:00Z",
  "embedding": [0.123, 0.456, ...],
  "access_count": 0,
  "last_accessed": null
}
```

#### Progressive Pre-Generation

**Don't wait for everything - start fast, expand in background:**

1. **Immediate**: Generate first 3-5 sections (instant start)
2. **Background**: Continue generating remaining sections while user watches
3. **On-demand**: Generate sections as user progresses through lesson
4. **Result**: Feels instant, but covers full content

#### Question Prediction & Pre-Answering

**Track and pre-answer common questions:**

1. **Track questions**: Log all user questions per lesson
2. **Identify patterns**: Find frequently asked questions
3. **Pre-generate answers**: Create video/audio responses for common questions
4. **Store in RAG**: Index with high priority for instant retrieval
5. **Result**: Instant answers to frequent questions

#### Edge Cases & Fallbacks

**Handle scenarios not covered by pre-generation:**

1. **Unexpected questions** â†’ Real-time generation fallback
2. **Follow-up questions** â†’ RAG finds related sections, chain them together
3. **Skip ahead requests** â†’ RAG finds relevant section, jump to it
4. **Content updates** â†’ Version RAG entries, invalidate old ones
5. **Cross-lesson questions** â†’ Search across all lessons in RAG

#### Performance Benefits

**This approach provides:**

- âœ… **Zero lag** for pre-generated content (instant retrieval)
- âœ… **Natural flow** (A/B alternation with seamless handoffs)
- âœ… **Contextual answers** (RAG finds relevant sections)
- âœ… **Progressive improvement** (more usage = more cached content = faster)
- âœ… **Handles common questions instantly** (pre-answered Q&A)
- âœ… **Graceful degradation** (real-time fallback for edge cases)

#### Implementation Priority

1. **Phase 1**: Basic pre-generation (sections A/B, store in RAG)
2. **Phase 2**: Metadata enrichment (keywords, concepts, topics)
3. **Phase 3**: Question prediction (track, pre-answer common questions)
4. **Phase 4**: Progressive generation (start fast, expand background)
5. **Phase 5**: Cross-lesson search (search across all lessons)

---

## Turn-Taking Logic

### Turn Engine Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Turn-Taking Engine Flow                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Initial State:
  speaker = leftTeacher
  renderer = rightTeacher
  queues[speaker].status = "idle"
  queues[renderer].status = "idle"

Step 1: Enqueue First Render Job
  â”‚
  â”œâ”€â–º POST /worker/{renderer_side}/run
  â””â”€â–º queues[renderer].status = "rendering"

Step 2: Worker Generates Clip
  â”‚
  â”œâ”€â–º Worker: RAG â†’ LLM â†’ TTS â†’ Video
  â””â”€â–º Worker: POST /session/:id/clip-ready
      â”‚
      â–¼
Step 3: Coordinator Receives CLIP_READY
  â”‚
  â”œâ”€â–º queues[renderer].status = "ready"
  â”œâ”€â–º queues[renderer].nextClipId = clipId
  â””â”€â–º Emit CLIP_READY event
      â”‚
      â–¼
Step 4: UI Plays Speaker Clip
  â”‚
  â”œâ”€â–º IF speaker has ready clip â†’ Play it
  â””â”€â–º ELSE â†’ Show "Rendering..." (shouldn't happen if timing is right)
      â”‚
      â–¼
Step 5: Clip Finishes Playing
  â”‚
  â”œâ”€â–º UI: POST /session/:id/speech-ended
  â””â”€â–º Payload: {sessionId, clipId}
      â”‚
      â–¼
Step 6: Coordinator Swaps Roles
  â”‚
  â”œâ”€â–º Validate: clipId matches current speaker clip
  â”œâ”€â–º IF renderer.status === "ready"
  â”‚  â”œâ”€â–º Swap: speaker â†” renderer
  â”‚  â”œâ”€â–º Increment turn
  â”‚  â”œâ”€â–º queues[new_speaker].status = "speaking"
  â”‚  â”œâ”€â–º queues[new_renderer].status = "idle"
  â”‚  â”œâ”€â–º Emit SPEAKER_CHANGED event
  â”‚  â””â”€â–º Enqueue render job for new renderer
  â”‚      â””â”€â–º POST /worker/{new_renderer_side}/run
  â””â”€â–º ELSE (renderer not ready)
     â”œâ”€â–º Emit WARNING event
     â””â”€â–º Generate bridging clip for current speaker
         â””â”€â–º Short filler (2-4 seconds): "Let me scroll to the next part..."

Step 7: Loop Continues
  â”‚
  â””â”€â–º Repeat from Step 2 with swapped roles
```

### Turn-Taking State Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Turn-Taking State Diagram                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[Session Start]
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Speaker: A      â”‚
â”‚  Renderer: B     â”‚
â”‚  B: rendering    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ B completes clip
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Speaker: A      â”‚
â”‚  Renderer: B     â”‚
â”‚  B: ready        â”‚
â”‚  A: speaking     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ A finishes speaking
         â”‚ POST speech-ended
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Speaker: B      â”‚ â—„â”€â”€â”€ SWAP
â”‚  Renderer: A     â”‚
â”‚  B: speaking     â”‚
â”‚  A: rendering    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ A completes clip
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Speaker: B      â”‚
â”‚  Renderer: A     â”‚
â”‚  A: ready        â”‚
â”‚  B: speaking     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ B finishes speaking
         â”‚ POST speech-ended
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Speaker: A      â”‚ â—„â”€â”€â”€ SWAP
â”‚  Renderer: B     â”‚
â”‚  A: speaking     â”‚
â”‚  B: rendering    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â””â”€â–º (Loop continues)
```

---

## Latency Masking Strategies

### Core Strategy: Always Stay One Clip Ahead

The renderer must **always** complete their clip before the speaker finishes. This creates the illusion of zero lag.

### Clip Length Targets

```
Speaker Clips:  5-12 seconds (optimal: 8-10s)
Renderer Clips: 8-20 seconds (optimal: 10-15s)
Bridging Clips: 2-4 seconds (emergency filler)
```

**Why different lengths?**
- Speaker clips are shorter for faster turn-taking
- Renderer clips can be longer since they're generated in parallel
- Longer renderer clips give more buffer time

### Timing Calculation

```
Expected speaker clip duration: 8 seconds
Expected renderer generation time: 12 seconds

Timeline:
T=0s:  Speaker starts playing clip (8s)
       Renderer starts generating (12s)
T=8s:  Speaker finishes
       Renderer should be ready (ideally completed at T=7s)
T=8s:  Swap roles, new speaker starts immediately
```

### Bridging Clip Strategy

If the renderer is not ready when the speaker finishes:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Bridging Clip Decision Tree        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Speaker finishes (POST speech-ended)
  â”‚
  â–¼
Is renderer.status === "ready"?
  â”œâ”€ YES â†’ Swap roles, play renderer clip
  â””â”€ NO â†’ Generate bridging clip
      â”‚
      â”œâ”€â–º Enqueue urgent render job for current speaker
      â”œâ”€â–º Generate short filler clip (2-4 seconds)
      â”‚   Examples:
      â”‚   - "Let me scroll to the next part..."
      â”‚   - "Okay, now watch this next section..."
      â”‚   - "Give me a moment to find that..."
      â”œâ”€â–º Play bridging clip
      â””â”€â–º Check renderer status again
          â”‚
          â”œâ”€â–º If ready â†’ Swap after bridging clip
          â””â”€â–º If still not ready â†’ Generate another bridging clip
```

### Adaptive Clip Length

The system can dynamically adjust clip length targets based on performance:

```
IF average_render_time > speaker_clip_duration:
  â”œâ”€â–º Reduce speaker clip target (5-8s instead of 8-12s)
  â”œâ”€â–º Increase renderer clip target (15-20s instead of 10-15s)
  â””â”€â–º Log performance warning

IF average_render_time < speaker_clip_duration * 0.7:
  â”œâ”€â–º Increase speaker clip target (10-15s instead of 8-12s)
  â””â”€â–º This allows more detailed explanations
```

---

## Failure Handling & Resilience

### Failure Mode Decision Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Failure Handling Strategy                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Video Generation Fails
   â”‚
   â”œâ”€â–º Retry: 2 attempts with exponential backoff
   â”œâ”€â–º IF still fails:
   â”‚  â”œâ”€â–º Send CLIP_READY with status="audio_only"
   â”‚  â”œâ”€â–º videoUrl = null or placeholder
   â”‚  â””â”€â–º UI shows avatar with idle animation + audio playback
   â””â”€â–º Log error for monitoring

2. TTS Generation Fails
   â”‚
   â”œâ”€â–º Retry: 2 attempts
   â”œâ”€â–º IF still fails:
   â”‚  â”œâ”€â–º Send ERROR event
   â”‚  â”œâ”€â–º Abort clip generation
   â”‚  â””â”€â–º Coordinator enqueues new render job
   â””â”€â–º Log critical error

3. LLM Generation Fails
   â”‚
   â”œâ”€â–º Retry: 1 attempt
   â”œâ”€â–º IF still fails:
   â”‚  â”œâ”€â–º Use fallback template response
   â”‚  â”‚   "Let me continue with the next section..."
   â”‚  â””â”€â–º Continue pipeline with fallback text
   â””â”€â–º Log warning

4. RAG Query Fails
   â”‚
   â”œâ”€â–º Continue without RAG context
   â”œâ”€â–º Log warning
   â””â”€â–º Teacher still generates response (may be less accurate)

5. Renderer Late (Not Ready When Speaker Finishes)
   â”‚
   â”œâ”€â–º Generate bridging clip for current speaker
   â”œâ”€â–º Play bridging clip (2-4 seconds)
   â”œâ”€â–º Check renderer status again
   â””â”€â–º IF still not ready â†’ Generate another bridging clip

6. Session State Stale (Job Invalid)
   â”‚
   â”œâ”€â–º Worker validates job before processing
   â”œâ”€â–º IF invalid:
   â”‚  â”œâ”€â–º Discard job silently
   â”‚  â”œâ”€â–º Return 200 OK (don't confuse UI)
   â”‚  â””â”€â–º Log debug message
   â””â”€â–º Coordinator enqueues fresh job

7. Network Failure (Worker Can't POST CLIP_READY)
   â”‚
   â”œâ”€â–º Retry: 3 attempts with exponential backoff
   â”œâ”€â–º IF still fails:
   â”‚  â”œâ”€â–º Store clip locally (if possible)
   â”‚  â”œâ”€â–º Log critical error
   â”‚  â””â”€â–º Coordinator will timeout and enqueue new job
   â””â”€â–º Monitoring system alerts on repeated failures

8. Coordinator API Down
   â”‚
   â”œâ”€â–º Workers queue jobs locally (if implemented)
   â”œâ”€â–º UI shows "Reconnecting..." message
   â”œâ”€â–º Retry connection with exponential backoff
   â””â”€â–º Restore session state when reconnected
```

### Graceful Degradation Levels

```
Level 1: Full Experience
  â”œâ”€â–º Video + Audio + Captions
  â””â”€â–º All features working

Level 2: Audio-Only Fallback
  â”œâ”€â–º Audio + Captions + Idle Animation
  â””â”€â–º Video generation failed

Level 3: Text-Only Fallback
  â”œâ”€â–º Captions only
  â””â”€â–º TTS failed (shouldn't happen, but possible)

Level 4: Bridging Mode
  â”œâ”€â–º Short filler clips
  â””â”€â–º Renderer consistently late

Level 5: Error State
  â”œâ”€â–º Show error message
  â”œâ”€â–º Allow user to retry or skip
  â””â”€â–º Log for debugging
```

---

## Implementation Checklist

### Phase 1: Core Infrastructure

- [ ] **Session State Schema**
  - [ ] Define session object structure
  - [ ] Implement state validation functions
  - [ ] Add state persistence (in-memory for now, DB later)

- [ ] **Coordinator API Endpoints**
  - [ ] `POST /session/start` - Create session, validate teachers
  - [ ] `GET /session/:id/state` - Get current session state
  - [ ] `POST /session/:id/section` - Update section snapshot
  - [ ] `POST /session/:id/speech-ended` - Notify clip finished
  - [ ] `POST /session/:id/clip-ready` - Worker posts completed clip
  - [ ] `GET /session/:id/events` - SSE event stream
  - [ ] `POST /rag/query` - RAG query endpoint

- [ ] **Event System**
  - [ ] Implement SSE server
  - [ ] Event emission functions
  - [ ] Event queue per session
  - [ ] Client connection management

### Phase 2: Frontend UI

- [ ] **Teacher Selection**
  - [ ] Multi-select component (exactly 2)
  - [ ] Validation (must select 2)
  - [ ] Teacher preview/description

- [ ] **Layout Components**
  - [ ] Left panel (Teacher A avatar)
  - [ ] Center panel (Website container)
  - [ ] Right panel (Teacher B avatar)
  - [ ] Bottom controls (captions, pause, speed, etc.)

- [ ] **Event Handling**
  - [ ] SSE client connection
  - [ ] Event listener/dispatcher
  - [ ] State management (session state, clip queues)
  - [ ] Auto-play logic (play speaker clip when ready)

- [ ] **Website Integration**
  - [ ] Iframe or embedded website
  - [ ] DOM extraction (visible text, selected text)
  - [ ] Scroll position tracking
  - [ ] Section change detection

- [ ] **Video/Audio Playback**
  - [ ] Video player component
  - [ ] Audio fallback (idle animation)
  - [ ] Caption display
  - [ ] Playback event handling (ended, error)

### Phase 3: n8n Workflows

- [ ] **SESSION_START Workflow**
  - [ ] Webhook trigger
  - [ ] Validate payload (2 teachers)
  - [ ] HTTP: Create session in Coordinator
  - [ ] HTTP: Enqueue first render job
  - [ ] Respond immediately (don't wait)

- [ ] **LEFT_WORKER Workflow**
  - [ ] Webhook trigger
  - [ ] HTTP: Get session state
  - [ ] IF: Validate job still valid
  - [ ] HTTP: Query RAG
  - [ ] Code: Prepare LLM prompt
  - [ ] HTTP: LLM Generate (Ollama)
  - [ ] Code: Extract & normalize response
  - [ ] HTTP: TTS Generate
  - [ ] HTTP: Video Generate (with retries)
  - [ ] Code: Format clip object
  - [ ] HTTP: POST CLIP_READY
  - [ ] Respond 200

- [ ] **RIGHT_WORKER Workflow**
  - [ ] Same as LEFT_WORKER (different route)

### Phase 4: Turn-Taking Engine

- [ ] **Swap Logic**
  - [ ] On speech-ended: validate renderer ready
  - [ ] Swap speaker/renderer
  - [ ] Update queue statuses
  - [ ] Emit SPEAKER_CHANGED event
  - [ ] Enqueue render job for new renderer

- [ ] **Bridging Clip Logic**
  - [ ] Detect renderer not ready
  - [ ] Generate short filler clip
  - [ ] Play bridging clip
  - [ ] Re-check renderer status

### Phase 5: RAG System

- [ ] **Knowledge Base Setup**
  - [ ] Chunking service
  - [ ] Embedding generation
  - [ ] Vector database (PostgreSQL + pgvector)
  - [ ] Index creation

- [ ] **RAG Query Service**
  - [ ] Query embedding generation
  - [ ] Vector similarity search
  - [ ] Result ranking and filtering
  - [ ] Context formatting for LLM

### Phase 6: Error Handling

- [ ] **Retry Logic**
  - [ ] Exponential backoff
  - [ ] Max retry limits
  - [ ] Retry on specific error types only

- [ ] **Fallback Mechanisms**
  - [ ] Audio-only fallback
  - [ ] Bridging clip generation
  - [ ] Error event emission

- [ ] **Monitoring**
  - [ ] Error logging
  - [ ] Performance metrics
  - [ ] Alerting on critical failures

---

## Tech Stack & Infrastructure

### Frontend
- **Framework**: Streamlit (Python) or React/Next.js
- **Video Playback**: HTML5 video element
- **Event Streaming**: Server-Sent Events (SSE) or WebSocket
- **DOM Extraction**: Browser APIs or headless browser

### Coordinator API
- **Framework**: FastAPI (Python) or Express.js (Node.js)
- **State Storage**: In-memory (development) â†’ PostgreSQL (production)
- **Event Streaming**: SSE (simpler) or WebSocket (more features)
- **RAG Service**: FastAPI endpoint with pgvector

### Automation
- **Platform**: n8n
- **Workflows**: 3 workflows (SESSION_START, LEFT_WORKER, RIGHT_WORKER)
- **Triggers**: Webhooks
- **HTTP Client**: Built-in HTTP Request nodes

### AI Services
- **LLM**: Ollama (mistral:7b) or OpenAI API
- **TTS**: Piper TTS or Coqui TTS (multi-language)
- **Avatar Video**: LongCat-Video-Avatar (talking head generation)
- **Embeddings**: OpenAI text-embedding-3-small or local model

### RAG System
- **Vector Database**: PostgreSQL + pgvector extension
- **Embedding Model**: OpenAI or local (sentence-transformers)
- **Chunking**: Semantic chunking with overlap
- **Index**: HNSW index for fast similarity search

### Storage
- **Instance Storage**: 500GB (code, environments, temporary files)
- **Storage Volume**: 1TB at `/workspace` (videos, logs, cache, database)
- **Video Storage**: `/workspace/data/videos/`
- **Audio Storage**: `/workspace/data/audio/`
- **Cache**: `/workspace/data/cache/` (content-based caching)
- **Logs**: `/workspace/logs/` (organized by service)
- **Database**: `/workspace/data/postgresql/` (PostgreSQL data directory)

### Deployment
- **Hosting**: VAST.AI GPU instances
- **GPUs**: 2x A100 (80GB VRAM total recommended)
- **Services**: All services run directly on host (no Docker for simplicity)
- **Process Management**: tmux sessions
- **Port Forwarding**: SSH tunnels from desktop to VAST instance

---

## Future Roadmap

### Short-Term (Next 3 Months)
- [ ] **Persistent RAG Memory**
  - [ ] Store session transcripts in RAG
  - [ ] Teachers can reference previous conversations
  - [ ] Cross-session learning

- [ ] **Semantic Section Detection**
  - [ ] Auto-detect section boundaries
  - [ ] Smart section transitions
  - [ ] Context-aware section assignment

- [ ] **Adaptive Clip Length**
  - [ ] Dynamic adjustment based on performance
  - [ ] Content-aware length optimization
  - [ ] User preference settings

### Medium-Term (3-6 Months)
- [ ] **Student Profiles**
  - [ ] Track student progress
  - [ ] Personalized teaching style
  - [ ] Learning path recommendations

- [ ] **Teacher Specialization**
  - [ ] Domain-specific teachers (math, coding, etc.)
  - [ ] Teaching style preferences
  - [ ] Teacher personality customization

- [ ] **Observability Dashboard**
  - [ ] Real-time system metrics
  - [ ] Performance monitoring
  - [ ] Error tracking and alerts

### Long-Term (6+ Months)
- [ ] **Teaching Presets**
  - [ ] Pre-configured teacher pairs
  - [ ] Lesson templates
  - [ ] Teaching mode presets (beginner, advanced, etc.)

- [ ] **Citation Mode**
  - [ ] Teachers cite sources from RAG
  - [ ] Show references on screen
  - [ ] Link to original content

- [ ] **Multi-Language Support**
  - [ ] Automatic language detection
  - [ ] Teacher language preferences
  - [ ] Real-time translation

- [ ] **Advanced RAG Features**
  - [ ] Multi-modal RAG (images, code, diagrams)
  - [ ] Temporal RAG (time-aware context)
  - [ ] Hierarchical RAG (lesson â†’ section â†’ concept)

---

## Project Philosophy

### Core Principles

1. **Never Block**
   - All operations are asynchronous
   - No waiting on other pipelines
   - Immediate responses to user actions

2. **Hide Latency**
   - Renderer always stays one clip ahead
   - Bridging clips mask any delays
   - Smooth transitions between teachers

3. **State-Driven**
   - Single source of truth (Coordinator)
   - Events communicate changes
   - No data merging or synchronization

4. **Resilient**
   - Graceful degradation at every layer
   - Multiple fallback mechanisms
   - Error recovery without user intervention

5. **Scalable**
   - Independent pipelines enable concurrency
   - Stateless workers (except session context)
   - Horizontal scaling possible

### Vision

This is not just avatars. This is a **distributed AI classroom** with:
- **Co-teaching**: Two teachers working together seamlessly
- **Memory**: RAG provides persistent knowledge
- **Grounding**: Teachers reference real content
- **Human Flow**: Natural turn-taking and handoffs

A system that **feels alive**.

---

## Appendix: Quick Reference

### Session State Schema
See [Session State Machine](#session-state-machine) section.

### Event Types
See [Event-Driven Communication](#event-driven-communication) section.

### API Endpoints
- `POST /session/start` - Create session
- `GET /session/:id/state` - Get session state
- `POST /session/:id/section` - Update section
- `POST /session/:id/speech-ended` - Notify clip finished
- `POST /session/:id/clip-ready` - Worker posts clip
- `GET /session/:id/events` - SSE event stream
- `POST /rag/query` - Query RAG system

### n8n Workflow Routes
- `/webhook/session/start` - SESSION_START workflow
- `/webhook/worker/left/run` - LEFT_WORKER workflow
- `/webhook/worker/right/run` - RIGHT_WORKER workflow

### Storage Paths
- Videos: `/workspace/data/videos/`
- Audio: `/workspace/data/audio/`
- Cache: `/workspace/data/cache/`
- Logs: `/workspace/logs/{service}/`
- Database: `/workspace/data/postgresql/`

---

**Document Version**: 2.0  
**Last Updated**: 2026-01-25  
**Status**: Active Development
